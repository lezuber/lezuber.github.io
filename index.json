[{"content":"Sources [[DTU: Introduction to Statistics]] Related Notes [[Discrete Distributions]]\n[[Continuous Distributions]]\n[[Notes]] Random variables Are the basic building block to describe random outcomes of an experiment. They are functions which assigns a numerical value to each outcome in the sample space. They are denoted with capital letters, e.g. $$X, Y, \u0026hellip;$$\nThey can be either discrete (e.g. dice roll) or continuous (e.g. time spent on homework each week).\nThe outcomes can be limited or unlimited. E.g. a dice roll is limited. But also continuous r.v. can be limited to e.g. only positive values such as weights, distances.\nDifference between discrete and continuous random variables\nIn the continuous case integrals are used, in the discrete case sums are used.\nIn the continuous case the probability of observing a single value is always zero. In the discrete case it can be positive or zero.\nDetails see below.\nDiscrete random variables e.g. the roll of a dice\nProbability density function (PDF) is defined as:\n$$f(x)=P(X=x)$$\nIt assigns a probability to every possible outcome value of $$x$$. A discrete $$pdf$$ fulfills two properties:\nThere are no negative probabilities for any outcome: $$f(x) \\geq 0$$ for all $$x$$\nAnd the probabilities of all outcomes sum to 1: $$\\sum_{\\text {all } x} f(x)=1$$\nCumulated distribution function (CDF) for a discrete case is the probability of realizing an outcome or equal to the value x: $$ F(x)=P(X \\leq x)=\\sum*{j \\text { where } x*{j} \\leq x} f\\left(x*{j}\\right)=\\sum*{j \\text { where } x*{j} \\leq x} P\\left(X=x*{j}\\right) $$\nThe probability that the outcome of X is in a range is $$P(a\u0026lt;X \\leq b)=F(b)-F(a)$$\nSo basically we are summing the PDF over the range of discrete values of desire. Hence, by definition it can be at most 1.\nExpected value Is the mean of a discrete random variable X.\n$$\\mu=\\mathrm{E}(X)=\\sum_{i=1}^{\\infty} x_{i} f\\left(x_{i}\\right)$$\nSo we sum up the all values times their probability. Recall that $$f(x_i)$$ sums to one. That\u0026rsquo;s why this works nicely. So it\u0026rsquo;s a weighted average.\nVariance It is the squared weighted average of all values of $$X$$ subtracted by the mean.\n$$\\mathrm{V}(X)=\\mathrm{E}\\left[(X-\\mu)^{2}\\right]=\\sum_{i=1}^{\\infty}\\left(x_{i}-\\mu\\right)^{2} f\\left(x_{i}\\right)$$\nIn contrast the standard deviation, it empathizes more on large variations, and less on small ones. (nature of the squaring)\nNote: It cannot be negative (because of the squaring)\nStandard deviation Is the square root of the variance\n$$\\sigma^{2}=\\mathrm{V}(X)$$\nThat way it is easier to describe, as it is on the same scale as the original values.\n[[Discrete Distributions]]\nContinuous Random variables heading:: true e.g. time spent on homework each week\nProbability density function (PDF) is defined as:\nThe area below the function of one: $$ \\int_{-\\infty}^{\\infty} f(x) d x=1 $$\nSo the probability of observing an outcome in the range $$a$$ to $$b$$ is: $$P(a\u0026lt;X \\leq b)=\\int_{a}^{b} f(x) d x$$\nNote that a single value has no area, hence we cannot give a probability for a single value in the continuous case, see: $$P(X=x)=P(x\u0026lt;X \\leq x)=\\int_{x}^{x} f(u) d u=0$$\nVisualization: + **Cumulated distribution function (CDF)** + We simply use a similar are as in the PDF, but only start on the left side in infinity: $$F(x)=P(X \\leq x)=\\int_{-\\infty}^{x} f(u) d u$$\n+ See how this can never be smaller than 0, and is non-decreasing. + **Expected Value** + $$\\mu=\\mathrm{E}(X)=\\int_{-\\infty}^{\\infty} x f(x) d x$$ + **Variance** + $$ \\sigma^{2}=\\mathrm{E}\\left[(X-\\mu)^{2}\\right]=\\int\\_{-\\infty}^{\\infty}(x-\\mu)^{2} f(x) d x $$ + [[Continuous Distributions]] How to calculate the Mean and Variance of linear combinations?\nHolds true for both discrete and continuous variables\n[[Linear Combinations of Random Variables]]\nQuestions ","permalink":"https://lezuber.github.io/pages/random-variables/","summary":"Sources [[DTU: Introduction to Statistics]] Related Notes [[Discrete Distributions]]\n[[Continuous Distributions]]\n[[Notes]] Random variables Are the basic building block to describe random outcomes of an experiment. They are functions which assigns a numerical value to each outcome in the sample space. They are denoted with capital letters, e.g. $$X, Y, \u0026hellip;$$\nThey can be either discrete (e.g. dice roll) or continuous (e.g. time spent on homework each week).\nThe outcomes can be limited or unlimited.","title":"Random Variables"},{"content":"Sources [[DTU: Introduction to Statistics]] Lecture 4\nKhan Academy Confidence intervals\nKhan Academy Z Statistics VS T-Statistics\nRelated Notes Random Variables\n[[Discrete Distributions]]\n[[Continuous Distributions]]\n[[Hypothesis Test]]\nMean and variance of repeating an experiment\nWhen you repeat an experiment with new data multiple times, you will always get different Summary Statistics results.\nBut the nice thing is, that such Mean and Variance will follow the normal distribution!\nCentral Limit Theorem (CLT) No matter what, the distribution of the mean becomes a Normal Distribution when n is large enough! So we can use our confidence intervals also with non-normal distributed data if $$n$$ is large!\nRule of thumb: $$n \u0026gt; 30$$\nNow, see how the expected value of a set of random samples is the population mean, not the sample mean! Should make sense, that the underlying mean is the true mean! $$ E(\\bar{X})=\\frac{1}{n} \\sum*{i=1}^{n} E\\left(X*{i}\\right)=\\frac{1}{n} \\sum_{i=1}^{n} \\mu=\\frac{1}{n} n \\mu=\\mu $$\nNow more interestingly: Similarly the variance of an experiment gets smaller with increasing repetitions of the experiment! $$\\operatorname{Var}(\\bar{X})=\\frac{1}{n^{2}} \\sum_{i=1}^{n} \\operatorname{Var}\\left(X_{i}\\right)=\\frac{1}{n^{2}} \\sum_{i=1}^{n} \\sigma^{2}=\\frac{1}{n^{2}} n \\sigma^{2}=\\frac{\\sigma^{2}}{n}$$\nHence, $$\\sigma = \\frac{\\sigma}{\\sqrt{n}}$$ Standard error\nFor this, we just subtract the value from the mean (how much \u0026ldquo;off\u0026rdquo; is the value from the mean), and divide it by the standard deviation! $$\\frac{\\bar{X}-\\mu}{\\sigma}$$ The only problem is, that we do not know $$\\sigma$$! Luckily we can approximate it by using the proof at the top of this note! Also as we learned above, this will follow a normal distribution for large $$n$$! $$\\frac{\\bar{X}-\\mu}{\\sigma / \\sqrt{n}} \\sim N\\left(0,1^{2}\\right)$$\nBut what for smaller values?\nThat\u0026rsquo;s where the t-Distribution comes in! This distribution represents the behavior much better for especially $$n\u0026lt;30$$, and approaches the normal distribution on larger $$n$$. Hence we can say: For small $$n$$: $$\\frac{\\bar{X}-\\mu}{\\sigma / \\sqrt{n}} \\sim N\\left(0,1^{2}\\right)$$ x-Standard error\nOften we use the terminology of e.g. 2-standard error, which means that we mean the interval that is $$2\\sigma$$ left and right of the mean $$\\mu$$. Confidence Interval\nGiven a experiment that we repeated $$n$$ times to get the mean of the n sample means of a distribution.\nNow we look at a single sample mean (just like in the Standard Error): What is the probability that this mean lies within e.g. 2-standard error of the actual mean?\nIt\u0026rsquo;s $$\\approx 95%$$ which can be looked up in tables. Note: Take either a Z-table or T-table depending on the size of n. Now, the confidence interval tells us in what range / interval the mean lies with e.g. 95% probability.\nTherefore, we calculate the standard error using our approximation proved at the top of the note:\n$$\\frac{\\sigma}{\\sqrt{n}}$$\nAnd, add and subtract it from the mean!\nThese two values are now our $$1\\sigma$$ confidence intervals. For other ones, just multiply the value by the interval you are searching for, and look the confidence up for that new interval.\nsee close connection to t-test and p-value [[Hypothesis Test]]\n[[QQ-Plot]]\nQuestions ","permalink":"https://lezuber.github.io/pages/statistics-of-samples/","summary":"Sources [[DTU: Introduction to Statistics]] Lecture 4\nKhan Academy Confidence intervals\nKhan Academy Z Statistics VS T-Statistics\nRelated Notes Random Variables\n[[Discrete Distributions]]\n[[Continuous Distributions]]\n[[Hypothesis Test]]\nMean and variance of repeating an experiment\nWhen you repeat an experiment with new data multiple times, you will always get different Summary Statistics results.\nBut the nice thing is, that such Mean and Variance will follow the normal distribution!\nCentral Limit Theorem (CLT) No matter what, the distribution of the mean becomes a Normal Distribution when n is large enough!","title":"statistics of samples"}]